{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "fpstalker_drawnapart.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU6-TF7Pq83n"
      },
      "source": [
        "!pip install ua_parser\n",
        "!pip install python-Levenshtein\n",
        "!pip install tensorflow-addons"
      ],
      "id": "NU6-TF7Pq83n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK-zxV3JvQAu"
      },
      "source": [
        "SHOULD_COMPUTE_EMBEDDINGS = False\n",
        "IS_DEMO = True"
      ],
      "id": "aK-zxV3JvQAu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiOz6Qrxq4U0"
      },
      "source": [
        "###############################################################\n",
        "###############################################################\n",
        "############# IMPORTS #########################################\n",
        "###############################################################"
      ],
      "id": "WiOz6Qrxq4U0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us5OSczWq4U3"
      },
      "source": [
        "from ua_parser import user_agent_parser\n",
        "import re\n",
        "import math as mt\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import numpy as np\n",
        "import datetime\n",
        "from Levenshtein import ratio as levenshtein_ratio\n",
        "from functools import reduce\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib \n",
        "import uuid\n",
        "from joblib import Parallel, delayed\n",
        "import json\n",
        "from statistics import mean\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "id": "us5OSczWq4U3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgphtdRZq4U5"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, losses\n",
        "import pickle\n",
        "import tensorflow_addons as tfa"
      ],
      "id": "dgphtdRZq4U5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkDP4KRhq4U5"
      },
      "source": [
        "###############################################################\n",
        "###############################################################\n",
        "############# MAIN CODE #######################################\n",
        "###############################################################"
      ],
      "id": "LkDP4KRhq4U5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKbARRpXq4U6"
      },
      "source": [
        "class Fingerprint():\n",
        "    ID = \"id\"\n",
        "    COUNTER = \"counter\"\n",
        "    CREATION_TIME = \"creationdate\"\n",
        "    END_TIME = \"enddate\"\n",
        "    \n",
        "    # HTTP attributes\n",
        "    ACCEPT_HTTP = \"accepthttp\"\n",
        "    LANGUAGE_HTTP = \"languagehttp\"\n",
        "    USER_AGENT_HTTP = \"useragenthttp\"\n",
        "    ADDRESS_HTTP = \"addresshttp\"\n",
        "    CONNECTION_HTTP = \"connectionhttp\"\n",
        "    ENCODING_HTTP = \"encodinghttp\"\n",
        "    HOST_HTTP = \"hosthttp\"\n",
        "    \n",
        "    BROWSER_FAMILY = \"browserFamily\"\n",
        "    MINOR_BROWSER_VERSION = \"minorBrowserVersion\"\n",
        "    MAJOR_BROWSER_VERSION = \"majorBrowserVersion\"\n",
        "    GLOBAL_BROWSER_VERSION = \"globalBrowserVersion\"\n",
        "    OS = \"os\"\n",
        "    \n",
        "    COOKIES_JS = \"cookiesjs\"\n",
        "    RESOLUTION_JS = \"resolutionjs\"\n",
        "    TIMEZONE_JS = \"timezonejs\"\n",
        "    PLUGINS_JS = \"pluginsjs\"\n",
        "    PLUGINS_JS_HASHED = \"pluginsjshashed\"\n",
        "    SESSION_JS = \"sessionjs\"\n",
        "    DNT_JS = \"dntjs\"\n",
        "    IE_DATA_JS = \"iedatajs\"\n",
        "    CANVAS_JS_HASHED = \"canvasjshashed\"\n",
        "    LOCAL_JS = \"localjs\"\n",
        "    PLATFORM_JS = \"platformjs\"\n",
        "    AD_BLOCK = \"adblock\"\n",
        "    RENDERER = \"rendererwebgljs\"\n",
        "    VENDOR = \"vendorwebgljs\"\n",
        "    \n",
        "    PLATFORM_INCONSISTENCY = \"platformInconsistency\"\n",
        "   \n",
        "    LANGUAGE_INCONSISTENCY = \"languageInconsistency\"\n",
        "    "
      ],
      "id": "cKbARRpXq4U6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD9Ca_1Vq4U7"
      },
      "source": [
        "LAMBDA = 0.992 \n",
        "WEBGL = True\n",
        "NB_DAYS = 7"
      ],
      "id": "cD9Ca_1Vq4U7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD1QLC31q4U8"
      },
      "source": [
        "\n",
        "\n",
        "JAVASCRIPT_ATTRIBUTES = [Fingerprint.COOKIES_JS, Fingerprint.RESOLUTION_JS, Fingerprint.TIMEZONE_JS,\n",
        "                             Fingerprint.PLUGINS_JS, Fingerprint.SESSION_JS, Fingerprint.DNT_JS, \n",
        "                             Fingerprint.CANVAS_JS_HASHED, Fingerprint.LOCAL_JS, Fingerprint.PLATFORM_JS,\n",
        "                             Fingerprint.PLATFORM_INCONSISTENCY, Fingerprint.IE_DATA_JS,\n",
        "                             Fingerprint.PLUGINS_JS_HASHED, Fingerprint.VENDOR, Fingerprint.RENDERER]\n",
        "\n",
        "REQUIRED_ATTRIBUTES = [Fingerprint.COUNTER, Fingerprint.COOKIES_JS, Fingerprint.RESOLUTION_JS, Fingerprint.TIMEZONE_JS,\n",
        "                       Fingerprint.PLUGINS_JS, Fingerprint.SESSION_JS, Fingerprint.DNT_JS, Fingerprint.ACCEPT_HTTP,\n",
        "                       Fingerprint.CANVAS_JS_HASHED, Fingerprint.LOCAL_JS, Fingerprint.PLATFORM_JS, Fingerprint.ENCODING_HTTP,\n",
        "                       Fingerprint.IE_DATA_JS, Fingerprint.PLUGINS_JS_HASHED, Fingerprint.VENDOR, Fingerprint.RENDERER,\n",
        "                      Fingerprint.END_TIME]\n",
        "\n",
        "RIGID_ATTRIBUTES = [Fingerprint.LOCAL_JS, \n",
        "                    Fingerprint.DNT_JS, \n",
        "                    Fingerprint.COOKIES_JS]\n",
        "\n",
        "MODEL_ATTRIBUTES = [\n",
        "    Fingerprint.CANVAS_JS_HASHED,\n",
        "    Fingerprint.CREATION_TIME,\n",
        "    Fingerprint.ID,\n",
        "    Fingerprint.LANGUAGE_HTTP,\n",
        "    Fingerprint.PLUGINS_JS,\n",
        "    Fingerprint.RENDERER,\n",
        "    Fingerprint.RESOLUTION_JS,\n",
        "    Fingerprint.TIMEZONE_JS,\n",
        "    Fingerprint.USER_AGENT_HTTP,\n",
        "]\n",
        "\n",
        "WEBGL_ATTRIBUTES = [\"embeddings_webgl\"]\n",
        "\n",
        "REQUIRED_ATTRIBUTES += list(set(MODEL_ATTRIBUTES + RIGID_ATTRIBUTES + WEBGL_ATTRIBUTES)) if WEBGL else list(set(MODEL_ATTRIBUTES + RIGID_ATTRIBUTES))\n",
        "\n",
        "def md5_encode(row, *attrs):\n",
        "    to_encode = \"\"\n",
        "    for attr in attrs:\n",
        "        if isinstance(attr, list):\n",
        "            for subattr in attr:\n",
        "                to_encode += str(row[subattr])\n",
        "        else:\n",
        "            to_encode += str(row[attr])\n",
        "        \n",
        "    return hashlib.md5(str.encode(to_encode)).hexdigest()\n",
        "\n",
        "def check_platform_inconsistencies(row):\n",
        "        if row[Fingerprint.TIMEZONE_JS] != \"no JS\":\n",
        "            try:\n",
        "                platform_user_agent = row[Fingerprint.OS][0:3].lower()\n",
        "                platform_js = row[Fingerprint.PLATFORM_JS][0:3].lower()\n",
        "                \n",
        "                if (platform_user_agent in (\"lin\", \"ubu\", \"ios\", \"and\")) and \".dll\" in row[Fingerprint.PLUGINS_JS]:\n",
        "                    return True\n",
        "                if platform_user_agent.startswith(\"ip\") and \"flash\" in row[Fingerprint.PLUGINS_JS].lower() > -1:\n",
        "                    return True   \n",
        "                if (platform_user_agent in (\"win\", \"mac\", \"ios\")) and \".so\" in row[Fingerprint.PLUGINS_JS] > -1:\n",
        "                    return True\n",
        "                if (platform_user_agent in (\"ubu\", \"win\", \"lin\")) and \".plugin\" in row[Fingerprint.PLUGINS_JS] > -1:\n",
        "                    return True\n",
        "                    \n",
        "                inconsistency = not(platform_js == platform_user_agent)\n",
        "                if platform_js == \"lin\" and platform_user_agent == \"and\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"lin\" and platform_user_agent == \"ubu\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"x64\" and platform_user_agent == \"win\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"ipa\" and platform_user_agent == \"ios\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"iph\" and platform_user_agent == \"ios\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"\" and platform_user_agent == \"\":\n",
        "                    inconsistency = True\n",
        "\n",
        "                elif platform_js == \"lin\" and platform_user_agent == \"and\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"lin\" and platform_user_agent == \"ubu\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"x64\" and platform_user_agent == \"win\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"ipa\" and platform_user_agent == \"ios\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"iph\" and platform_user_agent == \"ios\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"ipo\" and platform_user_agent == \"ios\":\n",
        "                    inconsistency = False\n",
        "                elif row[Fingerprint.OS] == \"Windows Phone\" and platform_js == \"arm\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"arm\" and \"SIM\" in row[Fingerprint.USER_AGENT_HTTP] > -1:\n",
        "                    inconsistency = False\n",
        "                elif platform_user_agent == \"chr\" and platform_js == \"lin\":\n",
        "                    inconsistency = False\n",
        "                elif \"Touch\" in row[Fingerprint.USER_AGENT_HTTP] > -1 and platform_js == \"arm\":\n",
        "                    inconsistency = False\n",
        "                elif platform_user_agent == \"oth\":\n",
        "                    inconsistency = False\n",
        "                elif platform_js == \"\" and platform_user_agent == \"\":\n",
        "                    inconsistency = True\n",
        "\n",
        "                return inconsistency\n",
        "            except:\n",
        "                return True\n",
        "        else:\n",
        "            raise ValueError(\"Javascript is not activated\")\n",
        "            \n",
        "\n",
        "def check_id_consistence(fps_df):\n",
        "    \n",
        "    # Various attributes checks\n",
        "    uid_inconsistencies = dict()\n",
        "    uid_fps_counter = dict()\n",
        "    for index, fp in fps_df.iterrows():\n",
        "        print(\"Handling index {}\".format(index), end=\"\\r\")\n",
        "        uid = fp.id\n",
        "        try:\n",
        "            fps_user = fps_df[(fps_df.id == uid) & (fps_df.creationdate <= fp.creationdate)]\n",
        "\n",
        "            browsers = set(fps_user[Fingerprint.BROWSER_FAMILY].values)\n",
        "            oses = set(fps_user[Fingerprint.OS].values)\n",
        "            if len(browsers) > 1 or len(oses) > 1:\n",
        "                uid_inconsistencies[uid] = 100000000\n",
        "            \n",
        "            if fp[Fingerprint.OS] in (\"Android\", \"iOS\", \"Windows Phone\", \"Firefox OS\", \"Windows 95\"):\n",
        "                uid_inconsistencies[uid] = 10000000000\n",
        "\n",
        "            if fp[Fingerprint.BROWSER_FAMILY] in (\"Safari\", \"IE\", \"Edge\", \"Googlebot\"):\n",
        "                uid_inconsistencies[uid] = 10000000\n",
        "            \n",
        "            if fp[Fingerprint.PLATFORM_INCONSISTENCY]:\n",
        "                if uid in uid_inconsistencies.keys():\n",
        "                    uid_inconsistencies[uid] += 5\n",
        "                else:\n",
        "                    uid_inconsistencies[uid] = 5\n",
        "\n",
        "            if not uid in uid_inconsistencies.keys():\n",
        "                uid_inconsistencies[uid] = 0\n",
        "            \n",
        "            if uid in uid_fps_counter.keys():\n",
        "                uid_fps_counter[uid] += 1\n",
        "            else:\n",
        "                uid_fps_counter[uid] = 1\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            uid_inconsistencies[uid] = 1000000\n",
        "    consistent_uids = []\n",
        "    for uid, counter in uid_fps_counter.items():\n",
        "        score = float(uid_inconsistencies[uid]) / float(counter)\n",
        "        if score < 0.02:\n",
        "            consistent_uids.append(uid)\n",
        "    print(\"Nb consistent\", len(consistent_uids))\n",
        "    # Canvas check\n",
        "    g = fps_df[[Fingerprint.ID, Fingerprint.CANVAS_JS_HASHED]].groupby(\"id\")\n",
        "    aggregated_canvas = g.agg([\"nunique\", \"count\"])\n",
        "    aggregated_canvas[\"rapport\"] = aggregated_canvas[Fingerprint.CANVAS_JS_HASHED][\"nunique\"]/aggregated_canvas[Fingerprint.CANVAS_JS_HASHED][\"count\"]\n",
        "    \n",
        "    poisoner_ids = aggregated_canvas[(aggregated_canvas[\"rapport\"] > 0.35) & (aggregated_canvas[Fingerprint.CANVAS_JS_HASHED][\"count\"] > 5)]\n",
        "    consistent_uids = list(filter(lambda x: x not in poisoner_ids.index.values, consistent_uids))\n",
        "    \n",
        "    return consistent_uids\n",
        "         \n",
        "def dataset_to_dataframe(fingerprint_dataset):\n",
        "    allowed_attributes = list(set(map(lambda x: x.lower(), REQUIRED_ATTRIBUTES)))\n",
        "    df = pd.DataFrame(fingerprint_dataset)[allowed_attributes]\n",
        "    \n",
        "    df[\"parsed_ua\"] = df[Fingerprint.USER_AGENT_HTTP].apply(lambda x: user_agent_parser.Parse(x))\n",
        "    df[Fingerprint.BROWSER_FAMILY] = df.parsed_ua.apply(lambda x: x[\"user_agent\"][\"family\"])\n",
        "    df[Fingerprint.MINOR_BROWSER_VERSION] = df.parsed_ua.apply(lambda x: x[\"user_agent\"][\"minor\"])\n",
        "    df[Fingerprint.MAJOR_BROWSER_VERSION] = df.parsed_ua.apply(lambda x: x[\"user_agent\"][\"major\"])\n",
        "    df[Fingerprint.GLOBAL_BROWSER_VERSION] = df[Fingerprint.MAJOR_BROWSER_VERSION] +  df[Fingerprint.MINOR_BROWSER_VERSION]\n",
        "    df[Fingerprint.OS] = df.parsed_ua.apply(lambda x: x[\"os\"][\"family\"])\n",
        "    \n",
        "    df[Fingerprint.PLATFORM_INCONSISTENCY] = df.apply(lambda row: check_platform_inconsistencies(row), axis=1)\n",
        "        \n",
        "    df[\"constant_hash\"] = df.apply(lambda row: md5_encode(row, Fingerprint.OS,\n",
        "                                                               Fingerprint.PLATFORM_JS,\n",
        "                                                               Fingerprint.BROWSER_FAMILY), axis=1)\n",
        "    \n",
        "    df[\"general_hash\"] = df.apply(lambda row: md5_encode(row, JAVASCRIPT_ATTRIBUTES,\n",
        "                                                              Fingerprint.USER_AGENT_HTTP,\n",
        "                                                              Fingerprint.LANGUAGE_HTTP,\n",
        "                                                              Fingerprint.ACCEPT_HTTP,\n",
        "                                                              Fingerprint.ENCODING_HTTP\n",
        "                                                        ), axis=1)\n",
        "    \n",
        "    return df\n",
        "            \n",
        "def generate_replay_sequence_pd(fps_df, visit_frequency):\n",
        "    uids = fps_df.id.unique()\n",
        "    uid_to_sequence = dict()\n",
        "    progress = 0\n",
        "    for uid in uids:\n",
        "        list_fps = fps_df[fps_df.id == uid]\n",
        "        if len(list_fps) > 1:\n",
        "            list_fps = list_fps.sort_values(\"creationdate\", ascending=True)\n",
        "            list_fps = list_fps.iloc[:-1]\n",
        "            \n",
        "            last_visit = list_fps.iloc[0].creationdate\n",
        "            dates = [last_visit]\n",
        "            sequence = []\n",
        "            \n",
        "            counter_str = \"{}_i\".format(list_fps.iloc[0].counter)\n",
        "            sequence.append((counter_str, last_visit))\n",
        "            \n",
        "            for _, fp in list_fps.iterrows():\n",
        "                variations_counter = 0\n",
        "                if not pd.isnull(fp[\"enddate\"]):\n",
        "                    if len(dates) > 0:\n",
        "                        last_visit = dates[-1] + datetime.timedelta(days=visit_frequency)\n",
        "                        \n",
        "                    frequency_str = \"{}D\".format(visit_frequency)\n",
        "                    dates = pd.date_range(last_visit, fp.enddate, freq=frequency_str, closed=\"left\").tolist()\n",
        "                    \n",
        "                    counters_str = [\"{}_{}\".format(fp.counter, variations_counter + index) \\\n",
        "                                    for index in range(len(dates))]\n",
        "                    current_sequence = [(cntr, date) for cntr, date in zip(counters_str, dates)]\n",
        "                    sequence += current_sequence\n",
        "                    variations_counter += len(dates) - 1\n",
        "            uid_to_sequence[uid] = sequence  \n",
        "        progress += 1\n",
        "        print(\"Handling uid {}/{}\".format(progress, len(uids)), end=\"\\r\")\n",
        "        \n",
        "    replay_sequence = list(itertools.chain(*uid_to_sequence.values()))\n",
        "    print(len(replay_sequence))\n",
        "    return sorted(replay_sequence, key=lambda x: x[1])\n",
        "\n",
        "\n",
        "def getTimeDifference(fp1, fp2):\n",
        "        try:\n",
        "            diff = fp1[Fingerprint.CREATION_TIME] - fp2[Fingerprint.CREATION_TIME]\n",
        "            return mt.fabs(diff.days + diff.seconds / (3600.0 * 24))\n",
        "        except:  # for the case where we try to link blink's fingerprints\n",
        "            return fp1[Fingerprint.COUNTER] - fp2[Fingerprint.COUNTER]\n",
        "\n",
        "def compute_similarity_vector(fp1, fp2, training=True):\n",
        "    similarity_vector = [] \n",
        "    \n",
        "    y = 0\n",
        "    if training and fp1[Fingerprint.ID] == fp2[Fingerprint.ID]:\n",
        "        y = 1\n",
        "    \n",
        "    time_difference = getTimeDifference(fp1, fp2)\n",
        "    similarity_vector.append(time_difference)\n",
        "    \n",
        "    \n",
        "    if fp1[Fingerprint.TIMEZONE_JS] == fp2[Fingerprint.TIMEZONE_JS]:\n",
        "        similarity_vector.append(1)\n",
        "    else:\n",
        "        similarity_vector.append(0)\n",
        "    \n",
        "    \n",
        "    if fp1[Fingerprint.RESOLUTION_JS] == fp2[Fingerprint.RESOLUTION_JS]:\n",
        "        similarity_vector.append(1)\n",
        "    else:\n",
        "        similarity_vector.append(0)\n",
        "        \n",
        "    if fp1[Fingerprint.CANVAS_JS_HASHED] == fp2[Fingerprint.CANVAS_JS_HASHED]:\n",
        "        similarity_vector.append(1)\n",
        "    else:\n",
        "        similarity_vector.append(0)\n",
        "        \n",
        "        \n",
        "        \n",
        "    already_tested = [Fingerprint.ID, Fingerprint.CANVAS_JS_HASHED, \n",
        "                      Fingerprint.RESOLUTION_JS, \n",
        "                      Fingerprint.TIMEZONE_JS, \n",
        "                      Fingerprint.CREATION_TIME, Fingerprint.COUNTER]\n",
        "    # I start this at one to take care of the fact that the creation time diff will always add one\n",
        "    nb_changes = reduce(lambda agg, curr: agg + 1 if curr == 0 else agg, similarity_vector, 1)\n",
        "    \n",
        "    \n",
        "    remaining_attributes = list(filter(lambda x: x not in already_tested, MODEL_ATTRIBUTES))\n",
        "    for attribute in remaining_attributes:\n",
        "        if fp1[attribute] != fp2[attribute]:\n",
        "            nb_changes += 1\n",
        "        similarity_ratio = levenshtein_ratio(fp1[attribute], fp2[attribute])\n",
        "        similarity_vector.append(similarity_ratio)\n",
        "        \n",
        "    \n",
        "    if not training and nb_changes > 5:\n",
        "        return np.array([]), np.array([])\n",
        "    \n",
        "    similarity_vector.append(nb_changes)\n",
        "            \n",
        "    return np.array(similarity_vector),  np.array([y])\n",
        "\n",
        "def random_sample(arr: np.array, avoid: str = None) -> np.array:\n",
        "    if avoid:\n",
        "        arr = arr[arr != avoid]\n",
        "    return arr[np.random.choice(len(arr), size=1, replace=False)][0]\n",
        "\n",
        "def train_ml(train_data, load=True, webgl_in_ml=False, model_path=\"./my_ml_model\"):\n",
        "    if load:\n",
        "        model = joblib.load(model_path)\n",
        "        return model\n",
        "    \n",
        "    X_train, y_train = [], []\n",
        "    for visit_frequency in range(1, 10):\n",
        "        print(\"Generating dataset for visit frequency of {} days\".format(visit_frequency))\n",
        "        replay_sequence = generate_replay_sequence_pd(train_data, visit_frequency)\n",
        "        train_counters = list(map(lambda x: int(x[0].split('_')[0]), replay_sequence))\n",
        "        \n",
        "        filtered_train_data = train_data[train_data[Fingerprint.COUNTER].isin(train_counters)]\n",
        "        \n",
        "        list_uids = filtered_train_data.id.unique()\n",
        "        for uid in list_uids:\n",
        "            uid_train_data = filtered_train_data[filtered_train_data.id == uid]\n",
        "            uid_train_data = uid_train_data.sort_values(Fingerprint.CREATION_TIME)\n",
        "            for i in range(1, len(uid_train_data)):\n",
        "                mod_attributes = MODEL_ATTRIBUTES + [WEBGL_ATTRIBUTE] if webgl_in_ml else MODEL_ATTRIBUTES\n",
        "                fp1 = uid_train_data[mod_attributes].iloc[i]\n",
        "                fp2 = uid_train_data[mod_attributes].iloc[i-1]\n",
        "                X, y = compute_similarity_vector(fp1, fp2)\n",
        "                if webgl_in_ml:\n",
        "                    X = np.concatenate((X, fp1[WEBGL_ATTRIBUTE], fp2[WEBGL_ATTRIBUTE]))\n",
        "                X_train.append(X)\n",
        "                y_train.append(y)\n",
        "            \n",
        "            for _, fp in uid_train_data.iterrows():\n",
        "                mod_attributes = MODEL_ATTRIBUTES + [WEBGL_ATTRIBUTE] if webgl_in_ml else MODEL_ATTRIBUTES\n",
        "                compared_uid = random_sample(list_uids, avoid=fp.id)\n",
        "                compared_uid_df = filtered_train_data[filtered_train_data.id == compared_uid][mod_attributes]\n",
        "                \n",
        "                compared_fp = compared_uid_df.iloc[random_sample(np.array(range(len(compared_uid_df))))][mod_attributes]\n",
        "                X, y = compute_similarity_vector(fp, compared_fp)\n",
        "                if webgl_in_ml:\n",
        "                    X = np.concatenate((X, fp[WEBGL_ATTRIBUTE], compared_fp[WEBGL_ATTRIBUTE]))\n",
        "                X_train.append(X)\n",
        "                y_train.append(y)\n",
        "        \n",
        "        \n",
        "    print(\"Generated dataset. Initiating model...\", end=\"\\r\")\n",
        "    model = RandomForestClassifier(n_jobs=6, random_state=2)\n",
        "    print(\"Initiated model. Training...\", end=\"\\r\")\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Trained model. Saving...\", end=\"\\r\")\n",
        "    joblib.dump(model, model_path)\n",
        "    print(\"Saved model. Training phase finished.\")\n",
        "\n",
        "    return model    "
      ],
      "id": "nD1QLC31q4U8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XHiNX9Mq4VI"
      },
      "source": [
        "def generate_id():\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "def check_attributes_consistency(infered_fp, compared_fp):\n",
        "    global RIGID_ATTRIBUTES\n",
        "    \n",
        "    if infered_fp[Fingerprint.GLOBAL_BROWSER_VERSION] < compared_fp[Fingerprint.GLOBAL_BROWSER_VERSION]:\n",
        "        return -1\n",
        "    \n",
        "    rigid_changes = reduce(lambda agg, curr_attr: agg | True if infered_fp[curr_attr] != compared_fp[curr_attr] else agg | False,\n",
        "                           RIGID_ATTRIBUTES,\n",
        "                           False)\n",
        "    if rigid_changes:\n",
        "        return -2\n",
        "    \n",
        "    return 0\n",
        "\n",
        "def compute_distance_embeddings(embedding_1, embedding_2, dist_type=\"cosine\"):\n",
        "    if dist_type == \"cosine\":\n",
        "        cos_sim = dot(embedding_1, embedding_2)/(norm(embedding_1)*norm(embedding_2))\n",
        "        return cos_sim\n",
        "    elif dist_type == \"euclidean\":\n",
        "        return np.linalg.norm(np.array(embedding_1) - np.array(embedding_2))\n",
        "    else:\n",
        "        raise Exception(\"Distance type not supported.\")\n",
        "        \n",
        "def infer_fingerprint(infered_fp, fps_df_dict, new_ids_df, threshold,  webgl_in_model=False, drawnapart=True):\n",
        "   \n",
        "    \n",
        "    potential_candidates = []\n",
        "    exact_candidates = []\n",
        "    \n",
        "    list_uids = new_ids_df.keys()\n",
        "    \n",
        "    for uid in list_uids:\n",
        "        for assigned_counter in new_ids_df[uid]:\n",
        "            known_counter_nbr = int(assigned_counter.split(\"_\")[0])\n",
        "            known_fp = fps_df_dict[known_counter_nbr]\n",
        "            if infered_fp.general_hash == known_fp[\"general_hash\"]:\n",
        "                exact_candidates.append({\n",
        "                    \"indexed_counter\": known_counter_nbr,\n",
        "                    \"linked_id\": uid\n",
        "                })\n",
        "            \n",
        "            elif len(exact_candidates) == 0 and infered_fp.constant_hash == known_fp[\"constant_hash\"]:\n",
        "                attributes_check = check_attributes_consistency(infered_fp, known_fp)\n",
        "                if attributes_check == -1: \n",
        "                    continue\n",
        "                elif attributes_check == -2:\n",
        "                    break\n",
        "                potential_candidates.append({\n",
        "                    \"indexed_counter\": known_counter_nbr,\n",
        "                    \"linked_id\": uid\n",
        "                })\n",
        "    \n",
        "                \n",
        "    if len(exact_candidates) > 0:\n",
        "        exact_uids = list(set(map(lambda x: x[\"linked_id\"], exact_candidates)))\n",
        "        if len(exact_uids) == 1:\n",
        "            return exact_uids[0]\n",
        "        \n",
        "    elif len(potential_candidates) > 0:\n",
        "        \n",
        "        # Inference over machine learning occurs here\n",
        "        updated_potential_candidates = []\n",
        "        X_infer = []\n",
        "        cosine_distances = []\n",
        "        for potential_candidate in potential_candidates:\n",
        "            known_fp = fps_df_dict[potential_candidate[\"indexed_counter\"]]\n",
        "            X, _ = compute_similarity_vector(infered_fp[MODEL_ATTRIBUTES], known_fp,  training=False)\n",
        "            \n",
        "            if len(X) > 0:\n",
        "                distance_embeddings = compute_distance_embeddings(infered_fp[\"embeddings_webgl\"], known_fp[\"embeddings_webgl\"])\n",
        "                if webgl_in_model:\n",
        "                    X = np.concatenate((X, infered_fp[WEBGL_ATTRIBUTE], known_fp[WEBGL_ATTRIBUTE]))\n",
        "                cosine_distances.append(distance_embeddings)\n",
        "                X_infer.append(X)\n",
        "                updated_potential_candidates.append(potential_candidate)\n",
        "                \n",
        "        if len(updated_potential_candidates) > 0:\n",
        "            inference = model.predict_proba(X_infer)\n",
        "            inference_positive_class = np.array(list(map(lambda x: x[1], inference)))\n",
        "\n",
        "            top_three = inference_positive_class.argsort()[::-1][:3]\n",
        "            \n",
        "            if drawnapart:\n",
        "                if cosine_distances[top_three[0]] > 0.15:\n",
        "                    return updated_potential_candidates[top_three[0]][\"linked_id\"]\n",
        "            \n",
        "            second_closest_index = 1\n",
        "            second_closest_probability = None\n",
        "            for index, closest_index in enumerate(top_three[1:]):\n",
        "                if inference_positive_class[top_three[0]] != inference_positive_class[closest_index]:\n",
        "                    second_closest_index = index + 1\n",
        "                    second_closest_probability = inference_positive_class[closest_index]\n",
        "                    break\n",
        "\n",
        "\n",
        "            is_difference_sufficient = True\n",
        "            if second_closest_probability and (inference_positive_class[top_three[0]] - second_closest_probability) < 0.3:\n",
        "                is_difference_sufficient = False\n",
        "\n",
        "            closest_candidates = list(set(map(lambda x: updated_potential_candidates[x][\"linked_id\"], top_three[:second_closest_index])))\n",
        "            if is_difference_sufficient and inference_positive_class[top_three[0]] > threshold and len(closest_candidates) == 1:\n",
        "                return updated_potential_candidates[top_three[0]][\"linked_id\"]\n",
        "            \n",
        "    \n",
        "    return generate_id()\n",
        "            \n",
        "\n",
        "def cleanup_fingerprints(generated_ids, counter_time, timestamp):\n",
        "    updated_generated_ids = {}\n",
        "    for assigned_id, counter_str in generated_ids.items():\n",
        "        tmp_time = counter_time[counter_str[-1]]\n",
        "        if timestamp - tmp_time <= pd.Timedelta(\"40 days\"):\n",
        "            updated_generated_ids[assigned_id] = counter_str\n",
        "    \n",
        "    return updated_generated_ids\n",
        "        \n",
        "        \n",
        "    \n",
        "def generate_replay_scenario(test_data, visit_frequency, model, webgl_in_model=False, drawnapart=True,\n",
        "                             save=\"./scenario_replay_result.csv\"):\n",
        "    \n",
        "    replay_sequence = generate_replay_sequence_pd(test_data, visit_frequency)\n",
        "    test_data = test_data.set_index(Fingerprint.COUNTER)\n",
        "    \n",
        "    fps_df_dict = test_data.to_dict('index')\n",
        "    \n",
        "    counter_time = dict((cntr, timestmp) for cntr, timestmp in replay_sequence)\n",
        "    generated_ids = {}\n",
        "    linked_ids = {}\n",
        "    for index, counter_object in enumerate(replay_sequence):\n",
        "        print(\"Step {}/{}\".format(index, len(replay_sequence)), end=\"\\r\")\n",
        "        counter_str, timestamp = counter_object\n",
        "        counter_nbr = int(counter_str.split(\"_\")[0])\n",
        "        \n",
        "        fp_to_infer = test_data[test_data.index.values == counter_nbr].iloc[0]\n",
        "        infered_id = infer_fingerprint(fp_to_infer, fps_df_dict, generated_ids, LAMBDA, webgl_in_model, drawnapart)\n",
        "        \n",
        "        linked_ids[counter_str] = infered_id\n",
        "        \n",
        "        if infered_id not in generated_ids.keys():\n",
        "            generated_ids[infered_id] = []\n",
        "        \n",
        "        generated_ids[infered_id] = generated_ids[infered_id][-1:]\n",
        "        generated_ids[infered_id].append(counter_str)\n",
        "    \n",
        "        if index % 2000 == 0:\n",
        "            generated_ids = cleanup_fingerprints(generated_ids, counter_time, timestamp)\n",
        "    return linked_ids"
      ],
      "id": "8XHiNX9Mq4VI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKLxiJQmq4VL"
      },
      "source": [
        "def evaluate_sequence(linked_ids, fps_df):\n",
        "    fps_df = fps_df.set_index(Fingerprint.COUNTER)\n",
        "    id_to_stats = {}\n",
        "    for counter_str, infered_id in linked_ids.items():\n",
        "        counter_nbr = int(counter_str.split(\"_\")[0])\n",
        "        fp_id = fps_df.loc[counter_nbr][Fingerprint.ID]\n",
        "        \n",
        "        if not fp_id in id_to_stats.keys():\n",
        "            id_to_stats[fp_id] = {\n",
        "                \"infered_id_set\": set(),\n",
        "                \"infered_ids\": {},\n",
        "                \"counters\": []\n",
        "            }\n",
        "            \n",
        "        id_to_stats[fp_id][\"infered_id_set\"].add(infered_id)\n",
        "        if infered_id not in id_to_stats[fp_id][\"infered_ids\"]:\n",
        "            id_to_stats[fp_id][\"infered_ids\"][infered_id] = []\n",
        "        id_to_stats[fp_id][\"infered_ids\"][infered_id].append(counter_nbr)\n",
        "        id_to_stats[fp_id][\"counters\"].append(counter_str)\n",
        "    \n",
        "    chain_lengths = {}\n",
        "    avg_chain_lengths = {}\n",
        "    perfect_track = {}\n",
        "    for uid in id_to_stats.keys():\n",
        "        tmp_lengths = {}\n",
        "        infered_ids = id_to_stats[uid][\"infered_id_set\"]\n",
        "        for infered_id in infered_ids:\n",
        "            nb_fps = list(filter(lambda x: fps_df.loc[x][Fingerprint.ID] == uid, id_to_stats[uid][\"infered_ids\"][infered_id]))\n",
        "            tmp_lengths[infered_id] = len(nb_fps)  \n",
        "        chain_lengths[uid] = max(tmp_lengths.items(), key=lambda x: x[1])[1]\n",
        "        avg_chain_lengths[uid] = mean(tmp_lengths.values())\n",
        "        perfect_track[uid] = len(id_to_stats[uid][\"counters\"])\n",
        "    return id_to_stats, chain_lengths, avg_chain_lengths, perfect_track"
      ],
      "id": "hKLxiJQmq4VL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czTxlo0hq4VM"
      },
      "source": [
        "###############################################################\n",
        "###############################################################\n",
        "############# LOADING CODE ####################################\n",
        "###############################################################"
      ],
      "id": "czTxlo0hq4VM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4uS5ZhqK8f5"
      },
      "source": [
        "!wget https://github.com/drawnapart/drawnapart/raw/master/bogus_dataset_for_fps.tgz\n",
        "!tar -zxvf bogus_dataset_for_fps.tgz"
      ],
      "id": "u4uS5ZhqK8f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMW7i_4dsTZx"
      },
      "source": [
        "df = pd.read_csv('bogus_dataset.tsv', sep='\\t')\n",
        "df['creationdate'] = pd.to_datetime(df['creationdate'])  \n",
        "df['enddate'] = pd.to_datetime(df['enddate'])  \n",
        "df['embeddings_webgl'] = df['embeddings_webgl'].apply(json.loads)\n",
        "\n",
        "print(\"Dataframe length:\", df.shape[0])"
      ],
      "id": "YMW7i_4dsTZx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3fdI4FMq4VN"
      },
      "source": [
        "def get_clf_model():\n",
        "    DROPOUT_SIZE = 0.119510\n",
        "\n",
        "    clf_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input((32, 32, 1)),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (4, 4), activation='relu'),\n",
        "    tf.keras.layers.Dropout(DROPOUT_SIZE),\n",
        "    tf.keras.layers.AveragePooling2D(),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (4, 4), activation='relu'),\n",
        "    tf.keras.layers.Dropout(DROPOUT_SIZE),\n",
        "    tf.keras.layers.AveragePooling2D(),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (4, 4), activation='relu'),\n",
        "    tf.keras.layers.Dropout(DROPOUT_SIZE),\n",
        "    tf.keras.layers.AveragePooling2D(),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "\n",
        "    tf.keras.layers.Dense(256, activation=None), # No activation on embeding dense layer\n",
        "    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)), # L2 normalize embeddings\n",
        "\n",
        "    tf.keras.layers.Dense(714, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    clf_model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['acc'])\n",
        "\n",
        "    return clf_model\n",
        "\n",
        "def get_triplet_model():\n",
        "    clf_model = get_clf_model()\n",
        "\n",
        "    triplet_model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.InputLayer((32, 32, 1))\n",
        "    ])\n",
        "\n",
        "    for layer in clf_model.layers[:-1]:\n",
        "        triplet_model.add(layer)\n",
        "\n",
        "    triplet_model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tfa.losses.TripletSemiHardLoss())\n",
        "\n",
        "    return triplet_model"
      ],
      "id": "e3fdI4FMq4VN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw0ES_28q4VO"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "def return_mean_embeddings(row):\n",
        "    array_embeddings = []\n",
        "    for trace_index in range(7):\n",
        "        array_embeddings.append(row[\"webgl_trace_{}\".format(trace_index)])\n",
        "    array_embeddings = np.mean(np.array(array_embeddings).reshape(7, 256), axis=0)\n",
        "    \n",
        "    return array_embeddings\n",
        "\n",
        "# In the bogus dataset, the embeddings are already computed\n",
        "if SHOULD_COMPUTE_EMBEDDINGS:\n",
        "  SCALER_PATH = './wild_scaler_for_clf_model.pkl'\n",
        "  WEIGHTS_PATH = './model_triplet_loss'\n",
        "\n",
        "  file_nm = f'{WEIGHTS_PATH}/weights.ckpt'\n",
        "  triplet_model = get_triplet_model()\n",
        "  triplet_model.load_weights(file_nm)\n",
        "\n",
        "  with open(SCALER_PATH, 'rb') as fd:\n",
        "    scaler = pickle.load(fd)\n",
        "  \n",
        "  for i in range(7):\n",
        "    df[\"webgl_trace_{}\".format(trace_index)] = scaler.transform(np.array(df[\"webgl_trace_{}\".format(trace_index)].to_list())).tolist()\n",
        "    df[\"webgl_trace_{}\".format(trace_index)] = df[\"webgl_trace_{}\".format(trace_index)].apply(lambda x: np.array(x).reshape(32, 32, 1)) \n",
        "    output_network = triplet_model.predict(np.array(df[\"webgl_trace_{}\".format(trace_index)].tolist()))\n",
        "    df[\"webgl_trace_{}\".format(trace_index)] = output_network.tolist()\n",
        "  \n",
        "  df[\"embeddings_webgl\"] = df.apply(lambda row: return_mean_embeddings(row), axis=1)"
      ],
      "id": "Nw0ES_28q4VO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzn8_RIDq4VP"
      },
      "source": [
        "###############################################################\n",
        "###############################################################\n",
        "############# EXECUTION #######################################\n",
        "###############################################################"
      ],
      "id": "lzn8_RIDq4VP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ms2NLF5q4VQ"
      },
      "source": [
        "%%time\n",
        "# Format df\n",
        "\n",
        "columns = df.columns\n",
        "lower_case = list(map(lambda x: x.lower(), columns))\n",
        "\n",
        "rename_dict = dict()\n",
        "for col, new_col in zip(columns, lower_case):\n",
        "    rename_dict[col] = new_col\n",
        "    \n",
        "df = df.rename(columns=rename_dict)\n",
        "formatted_df = dataset_to_dataframe(df)"
      ],
      "id": "4Ms2NLF5q4VQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mrv_tUYq4VQ"
      },
      "source": [
        "%%time\n",
        "\n",
        "if not IS_DEMO:\n",
        "  consistent_uids = check_id_consistence(formatted_df)\n",
        "else:\n",
        "  consistent_uids = formatted_df['id'].unique()"
      ],
      "id": "_Mrv_tUYq4VQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCCAe7XCq4VR"
      },
      "source": [
        "formatted_df[\"cnt_uids\"] = df.groupby('id')['collection_id'].transform('count')\n",
        "\n",
        "if not IS_DEMO:\n",
        "  filtered_df = formatted_df[formatted_df.cnt_uids > 6]\n",
        "  filtered_df = filtered_df[filtered_df[Fingerprint.ID].isin(consistent_uids)]\n",
        "  filtered_df = filtered_df.sort_values(\"counter\")\n",
        "else:\n",
        "  filtered_df = formatted_df"
      ],
      "id": "iCCAe7XCq4VR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggXijcUHq4VR"
      },
      "source": [
        "#Training\n",
        "\n",
        "if not IS_DEMO:\n",
        "  train_df = filtered_df[filtered_df.creationdate < '2021-02-07']\n",
        "  test_df = filtered_df[filtered_df.creationdate >= '2021-05-03']\n",
        "else:\n",
        "  train_df = filtered_df[filtered_df.creationdate < '2020-02-20']\n",
        "  test_df = filtered_df[filtered_df.creationdate >= '2020-02-20']\n",
        "\n",
        "train_df = train_df.sort_values(Fingerprint.CREATION_TIME)\n",
        "test_df = test_df.sort_values(Fingerprint.CREATION_TIME)\n",
        "test_df[Fingerprint.COUNTER] = test_df[Fingerprint.COUNTER].astype(\"int32\")"
      ],
      "id": "ggXijcUHq4VR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WE1nVKq4VS"
      },
      "source": [
        "%%time\n",
        "model = train_ml(train_df, False, webgl_in_ml=False)"
      ],
      "id": "y7WE1nVKq4VS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQxqpnGq4VS"
      },
      "source": [
        "###### INFERENCE ######"
      ],
      "id": "kJQxqpnGq4VS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW14A6iHMo_g"
      },
      "source": [
        "train_df.shape[0], test_df.shape[0]"
      ],
      "id": "VW14A6iHMo_g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAJH7rWSq4VT"
      },
      "source": [
        "%%time\n",
        "result = generate_replay_scenario(test_df, NB_DAYS, model, webgl_in_model=False, drawnapart=True)"
      ],
      "id": "jAJH7rWSq4VT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9y4UUKiq4VT"
      },
      "source": [
        "evaluation_obj, chain_result, avg_chain_result, perfect_track = evaluate_sequence(result, test_df)\n",
        "print(\"Chain result (max number of elements linked by one id)\", json.dumps(chain_result, indent=4, sort_keys=True))"
      ],
      "id": "P9y4UUKiq4VT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBQFwzSOq4VU"
      },
      "source": [
        "from matplotlib.pyplot import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def log_10_product(x, pos):\n",
        "    \"\"\"The two args are the value and tick position.\n",
        "    Label ticks with the product of the exponentiation\"\"\"\n",
        "    return '%1i' % (x)\n",
        "\n",
        "def plot_and_save(chain_result, avg_chain_result, nb_days):\n",
        "    stats_unicity = nb_days* np.array(sorted(chain_result.values()))\n",
        "    p1 = 1. * np.arange(len(chain_result)) / (len(chain_result) - 1)\n",
        "    p1 = 1- np.array(p1)\n",
        "\n",
        "    stats_avg = nb_days* np.array(sorted(avg_chain_result.values()))\n",
        "    p2 = 1. * np.arange(len(avg_chain_result)) / (len(avg_chain_result) - 1)\n",
        "    p2 = 1- np.array(p2)\n",
        "\n",
        "\n",
        "    median_max_track = np.median(nb_days * np.array(list(chain_result.values())))\n",
        "    median_avg_track = np.median(nb_days * np.array(list(avg_chain_result.values())))\n",
        "    avg_max_track = np.mean(nb_days * np.array(list(chain_result.values())))\n",
        "\n",
        "    axes = plt.gca()\n",
        "    l1 = plt.plot(stats_unicity, p1, color=\"green\", label=\"No of unique rules\")\n",
        "    l2 = plt.plot(stats_avg, p2, color=\"red\", label=\"No of unique rules\")\n",
        "    \n",
        "    axes.grid(True)\n",
        "    formatter = FuncFormatter(log_10_product)\n",
        "    axes.xaxis.set_major_formatter(formatter)\n",
        "    plt.title(\"FPStalker output (median (MAX) {} - median (AVG) {} - mean(AVG) {})\".format(median_max_track, median_avg_track, avg_max_track))\n",
        "    plt.xlabel('Tracking duration (collect frequency = {} days)'.format(nb_days), fontsize=14)\n",
        "    plt.ylabel('Browser instances (%)', fontsize=14)\n",
        "  \n",
        "    plt.show()\n",
        "\n",
        "plot_and_save(chain_result, avg_chain_result, NB_DAYS)"
      ],
      "id": "GBQFwzSOq4VU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AhbzT0Tq4VU"
      },
      "source": [
        "############################################################\n",
        "############################################################\n",
        "################## UTILS ###################################\n",
        "############################################################\n",
        "############################################################"
      ],
      "id": "7AhbzT0Tq4VU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDu0T53q4VU"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def compute_distance_top_left(tpr, fp):\n",
        "    return (0 - fp) * (0 - fp) + (1 - tpr) * (1 - tpr)\n",
        "\n",
        "def optimize_lambda(train_data, test_data):\n",
        "    X, y = [], []\n",
        "    for visit_frequency in range(1, 10):\n",
        "        print(\"Handling visit frequency {}/10\".format(visit_frequency))\n",
        "        train_replay_sequence = generate_replay_sequence_pd(train_data, visit_frequency)\n",
        "        train_counters = list(map(lambda x: int(x[0].split('_')[0]), train_replay_sequence))\n",
        "        \n",
        "        filtered_train_data = train_data[train_data[Fingerprint.COUNTER].isin(train_counters)]\n",
        "        \n",
        "        list_uids = filtered_train_data.id.unique()\n",
        "        \n",
        "        for uid in list_uids:\n",
        "            uid_train_data = filtered_train_data[filtered_train_data.id == uid]\n",
        "            uid_train_data = uid_train_data.sort_values(Fingerprint.CREATION_TIME)\n",
        "    \n",
        "            for i in range(1, len(uid_train_data)):\n",
        "                mod_attributes = MODEL_ATTRIBUTES\n",
        "                fp1 = uid_train_data[mod_attributes].iloc[i]\n",
        "                fp2 = uid_train_data[mod_attributes].iloc[i-1]\n",
        "                X_row, y_row = compute_similarity_vector(fp1, fp2)\n",
        "                X.append(X_row)\n",
        "                y.append(y_row)\n",
        "        \n",
        "            for _, fp in uid_train_data.iterrows():\n",
        "                mod_attributes = MODEL_ATTRIBUTES\n",
        "                compared_uid = random_sample(list_uids, avoid=fp.id)\n",
        "                compared_uid_df = filtered_train_data[filtered_train_data.id == compared_uid][mod_attributes]\n",
        "                \n",
        "                compared_fp = compared_uid_df.iloc[random_sample(np.array(range(len(compared_uid_df))))][mod_attributes]\n",
        "                X_row, y_row = compute_similarity_vector(fp, compared_fp)\n",
        "                X.append(X_row)\n",
        "                y.append(y_row)\n",
        "    \n",
        "    model = RandomForestClassifier(n_jobs=-1)\n",
        "    print(\"Training data: %d\" % len(X))\n",
        "    model.fit(X, y)\n",
        "    print(\"Finished training\")\n",
        "    \n",
        "    y_true = []\n",
        "    y_scores = []\n",
        "    for visit_frequency in range(1, 20):\n",
        "        print(\"Handling visit frequency {}/20\".format(visit_frequency))\n",
        "        test_replay_sequence = generate_replay_sequence_pd(test_data, visit_frequency)\n",
        "        test_counters = list(map(lambda x: int(x[0].split('_')[0]), test_replay_sequence))\n",
        "        \n",
        "        filtered_test_data = test_data[test_data[Fingerprint.COUNTER].isin(test_counters)]\n",
        "        \n",
        "        list_uids = filtered_test_data.id.unique()\n",
        "        x_rows = []\n",
        "        for uid in list_uids:\n",
        "            uid_test_data = filtered_test_data[filtered_test_data.id == uid]\n",
        "            uid_test_data = uid_test_data.sort_values(Fingerprint.CREATION_TIME)\n",
        "    \n",
        "            for i in range(1, len(uid_test_data)):\n",
        "                mod_attributes = MODEL_ATTRIBUTES\n",
        "                fp1 = uid_test_data[mod_attributes].iloc[i]\n",
        "                fp2 = uid_test_data[mod_attributes].iloc[i-1]\n",
        "                X_row, y_row = compute_similarity_vector(fp1, fp2)\n",
        "                x_rows.append(X_row)\n",
        "                y_true.append(1)\n",
        "        \n",
        "            for _, fp in uid_test_data.iterrows():\n",
        "                mod_attributes = MODEL_ATTRIBUTES\n",
        "                compared_uid = random_sample(list_uids, avoid=fp.id)\n",
        "                compared_uid_df = filtered_test_data[filtered_test_data.id == compared_uid][mod_attributes]\n",
        "                \n",
        "                compared_fp = compared_uid_df.iloc[random_sample(np.array(range(len(compared_uid_df))))][mod_attributes]\n",
        "                X_row, y_row = compute_similarity_vector(fp, compared_fp)\n",
        "                x_rows.append(X_row)\n",
        "                y_true.append(0)\n",
        "                \n",
        "        predictions = model.predict_proba(x_rows)\n",
        "        for prediction in predictions:\n",
        "            y_scores.append(prediction[1])\n",
        "    \n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_scores, pos_label=1)\n",
        "    min_indice = 0\n",
        "    min_distance = compute_distance_top_left(tpr[0], fpr[0])\n",
        "    for i in range(1, len(fpr)):\n",
        "        distance = compute_distance_top_left(tpr[i], fpr[i])\n",
        "        if distance < min_distance:\n",
        "            min_indice = i\n",
        "            min_distance = distance\n",
        "    print(\"best point\")\n",
        "    print(\"%f, %f, %f\" % (fpr[min_indice], tpr[min_indice], thresholds[min_indice]))\n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=lw)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=15)\n",
        "    plt.ylabel('True Positive Rate', fontsize=15)\n",
        "    plt.savefig(\"./figures/lambda_optim.pdf\")\n",
        "    plt.show()"
      ],
      "id": "BJDu0T53q4VU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tz7WHAYq4VV"
      },
      "source": [
        "optimize_lambda(train_df, test_df)"
      ],
      "id": "-Tz7WHAYq4VV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VojtfqELq4VV"
      },
      "source": [
        "###### Running as a loop\n",
        "\n",
        "def run_loop(test_df, collect_freq=[], da=True):\n",
        "    for freq in collect_freq:\n",
        "        if da:\n",
        "            exp_name = \"collect_freq_with_da_3rd_may_{}_days_secondtrial\".format(freq)\n",
        "        else:\n",
        "            exp_name = \"collect_freq_without_da_3rd_may_{}_days_secondtrial\".format(freq)\n",
        "        print(\"Starting collection experiment with frequency {} days\".format(freq))\n",
        "        result_freq = generate_replay_scenario(test_df, freq, model, top_7_canvas=None, webgl=False, da=da,\n",
        "                            save=\"./scenario_replay_result.csv\")\n",
        "        with open('outputs/fp_result/result_complete_{}.pickle'.format(exp_name), 'wb') as handle:\n",
        "            pickle.dump(result_freq, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "            \n",
        "        evaluation_obj, chain_result, avg_chain_result, _ = evaluate_sequence(result_freq, test_df)\n",
        "        with open('outputs/evaluation/evaluation_object_complete_{}.pickle'.format(exp_name), 'wb') as handle:\n",
        "            pickle.dump(evaluation_obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open('outputs/evaluation/chain_result_complete_{}.pickle'.format(exp_name), 'wb') as handle:\n",
        "            pickle.dump(chain_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "        plot_and_save(chain_result, avg_chain_result, exp_name, freq)\n",
        "            "
      ],
      "id": "VojtfqELq4VV",
      "execution_count": null,
      "outputs": []
    }
  ]
}